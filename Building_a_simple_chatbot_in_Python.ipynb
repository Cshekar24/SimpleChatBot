{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zj9h92wUDPN"
   },
   "source": [
    "**Importing the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4BmyocFbb0MJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np  #For numerical computation in python\n",
    "import nltk         #For natural language processing\n",
    "import string       #process strings in python\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85QE5FDSUKqU"
   },
   "source": [
    "**Importing and reading the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jouIkYEkb9Pk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chandru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Chandru\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "f=open('chatbotDS.txt','r',errors = 'ignore')  #r-raw document\n",
    "raw_doc=f.read()\n",
    "raw_doc=raw_doc.lower() #Converts text to lowercase\n",
    "nltk.download('punkt') #Using the Punkt tokenizer. Other tokenizers inlcude tweek, RegEx etc\n",
    "nltk.download('wordnet') #Using the WordNet dictionary\n",
    "sent_tokens = nltk.sent_tokenize(raw_doc) #Converts doc to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw_doc) #Converts doc to list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Corpus is taken from a wikipedia page\n",
    "https://en.wikipedia.org/wiki/Data_science "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#More about tokenizers: <br>\n",
    "https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmXgGkVeUSUb"
   },
   "source": [
    "**Example of sentance tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Swu4WRVncPR8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains.',\n",
       " 'data science is related to data mining, machine learning and big data.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:2]  #Printing first two sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtkzd0KhUWJT"
   },
   "source": [
    "**Example of word tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hcwrvmWicaLc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'is',\n",
       " 'an',\n",
       " 'interdisciplinary',\n",
       " 'field',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'scientific',\n",
       " 'methods']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:10]  #Print first 10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvvYcZZ9UbVD"
   },
   "source": [
    "**Text preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YbZllVqBcc78"
   },
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK library.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLX8WBE4UgOr"
   },
   "source": [
    "**Defining the greeting function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dLOqphibchJM"
   },
   "outputs": [],
   "source": [
    "GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")   #sup is Millenial shortform for what's up?\n",
    "GREET_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greet(sentence): \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREET_INPUTS:\n",
    "            return random.choice(GREET_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJhFmyRCUm4j"
   },
   "source": [
    "**Response generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eo7Kv52HcjG0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  #Term frequency and inverse document frequency(for rare words)\n",
    "from sklearn.metrics.pairwise import cosine_similarity       #It gives normalized vectors to the machine for it to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JEHZesw3cnNM"
   },
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "  robo1_response=''\n",
    "  TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "  idx=vals.argsort()[0][-2]\n",
    "  flat = vals.flatten()\n",
    "  flat.sort()\n",
    "  req_tfidf = flat[-2]\n",
    "  if(req_tfidf==0):\n",
    "    robo1_response=robo1_response+\"I am sorry! I don't understand you\"\n",
    "    return robo1_response\n",
    "  else:\n",
    "    robo1_response = robo1_response+sent_tokens[idx]\n",
    "    return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q-iY_o1Utas"
   },
   "source": [
    "**Defining conversation start/end protocols**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wxzENVDgdNGd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: My name is Chand. Let's have a conversation! Also, if you want to exit any time, just type Bye!\n",
      "hello\n",
      "BOT: hey\n",
      "hi\n",
      "BOT: *nods*\n",
      "foundations\n",
      "BOT: [4][5]\n",
      "\n",
      "\n",
      "contents\n",
      "1\tfoundations\n",
      "1.1\trelationship to statistics\n",
      "2\tetymology\n",
      "2.1\tearly usage\n",
      "2.2\tmodern usage\n",
      "3\timpact\n",
      "4\ttechnologies and techniques\n",
      "4.1\ttechniques\n",
      "5\tsee also\n",
      "6\treferences\n",
      "foundations\n",
      "data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains.\n",
      "impact of data science\n",
      "BOT: \"how data science will impact future of businesses?\".\n",
      "data science\n",
      "BOT: \"data science\".\n",
      "data science\n",
      "BOT: \"data science\".\n",
      "technologies and techniques\n",
      "BOT: [29]\n",
      "\n",
      "technologies and techniques\n",
      "there is a variety of different technologies and techniques that are used for data science which depend on the application.\n",
      "background\n",
      "BOT: I am sorry! I don't understand you\n",
      "bye\n",
      "BOT: Goodbye! Take care <3 \n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"BOT: My name is Chand. Let's have a conversation! Also, if you want to exit any time, just type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"BOT: You are welcome..\")\n",
    "        else:\n",
    "            if(greet(user_response)!=None):\n",
    "                print(\"BOT: \"+greet(user_response))\n",
    "            else:\n",
    "                sent_tokens.append(user_response)\n",
    "                word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
    "                final_words=list(set(word_tokens))\n",
    "                print(\"BOT: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"BOT: Goodbye! Take care <3 \")  #<3 is for heart shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5yXpRQMXopU"
   },
   "source": [
    "**This is one of the most simple chatbots you can build with very few lines of code. Of course, if you want more sophisticated chatbot then it all depends on the scale & vastness of the corpus which we give for training. Hope this helps my fellow friends and Data Science aspirants.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have learnt the chatbot development\n"
     ]
    }
   ],
   "source": [
    "print(\"You have learnt the chatbot development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its the end. Happy Learning\n"
     ]
    }
   ],
   "source": [
    "print(\"Its the end. Happy Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Building a chatbot in Python",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
